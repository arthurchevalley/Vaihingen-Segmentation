{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "84zwqQLaWdPy"
      },
      "source": [
        "# HyperColumns for Vaihingen dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yE5UtjMZWdP4"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VLS1m-_-WdP5"
      },
      "source": [
        "### 1.1 Install dependencies\n",
        "\n",
        "Please re-run this as we now need another package (for downloading the data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHmP6CaDWdP6",
        "outputId": "9ca74ba0-c138-48db-9ead-da5a825be28b"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install torch torchvision\n",
        "!{sys.executable} -m pip install matplotlib\n",
        "# Nice progress bar\n",
        "!{sys.executable} -m pip install tqdm                      \n",
        "\n",
        "# Albumentation for data augmentation\n",
        "!{sys.executable} -m pip install -U git+https://github.com/albumentations-team/albumentations\n",
        "\n",
        "# for downloading files from Google drive\n",
        "!{sys.executable} -m pip install gdown\n",
        "\n",
        "!{sys.executable} -m pip install --upgrade opencv-python\n",
        "\n",
        "import glob\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AzhqRXyPWdP8"
      },
      "source": [
        "### 1.2 Check if GPU available\n",
        "\n",
        "It is highly recommanded to have access to a GPU as the segmentation requires big computations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mj6A7RGWdP9",
        "outputId": "ff58de3a-e113-4499-9d04-189bafa4f07b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pa7Dc8MuWdP9"
      },
      "source": [
        "### 1.3 Random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2pLffznkWdP-"
      },
      "outputs": [],
      "source": [
        "seed = 324533\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a3QWS6mDWdQA"
      },
      "source": [
        "## 2. Dataset\n",
        "\n",
        "For this project the ISPRS Vaihingen semantic segmentation dataset will be used.\n",
        "This is a set of fully-labelled satellite image-segmentation mask pairs, with 9cm resolution and six land cover classes: Impervious, Buildings, Low Vegetation, Tree, Car, Clutter. The images come from a large satellite scene over the town Vaihingen in Germany and were divided into 33 patches, some of which are available with ground truth. As those patches are quite big they have been reduced in multiple 512x512 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfhsIBrZmO90",
        "outputId": "e8b49eec-eaa1-4c08-d8f2-abd3e071aa2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1S8oCD1fK4_l2L6lYwuHOHcNtKhRYg19b\n",
            "To: /content/vaihingen_512x512_full.tar.gz\n",
            "100% 853M/853M [00:03<00:00, 258MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1S8oCD1fK4_l2L6lYwuHOHcNtKhRYg19b\n",
        "\n",
        "!tar -xf vaihingen_512x512_full.tar.gz\n",
        "data_root = 'dataset_512x512_full'\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BxlN4qJmwGSg"
      },
      "source": [
        "## 2. Transformations for data augmentation & Dataset Init"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YiDFKyTNjdbl"
      },
      "source": [
        "As the dataset is not that big, some data augmentation has been made. The images having 5 channels and a segmentation mask, the basic PyTorch augmentations cannot be used. Hence another library which can handle this kind of images has been used. This is the [Albumentation](https://albumentations.ai/) library.\n",
        "\n",
        "Three set of augmentations exists: \n",
        "* One for training which crop the images to 400x400 pixels and apply diverse rotation with a probability of 50 %\n",
        "* One for validating which only apply a cropping to match sizes\n",
        "* A final one for the report analysis which does not do any augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1n3aHPqv1dZW"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from albumentations import VerticalFlip, Transpose, HorizontalFlip, RandomRotate90, RandomCrop\n",
        "crop_height = 400\n",
        "crop_width = 400\n",
        "train_transform = A.Compose(\n",
        "    [\n",
        "        RandomCrop(height=crop_height, width=crop_width),\n",
        "        VerticalFlip(p=0.5),\n",
        "        RandomRotate90(p=0.5),\n",
        "        Transpose(p=0.5),\n",
        "        HorizontalFlip(p=0.5),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transform = A.Compose(\n",
        "    [\n",
        "        A.RandomCrop(height=crop_height, width=crop_width),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "report_transform = A.Compose(\n",
        "    [\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SccO4eZqwSck"
      },
      "source": [
        "Dataset initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "wULgvrKNWdQE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as T      # transformations that can be used e.g. for data conversion or augmentation\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class VaihingenDataset(dataset.Dataset):\n",
        "    '''\n",
        "        Custom Dataset class that loads images and ground truth segmentation\n",
        "        masks from a directory.\n",
        "    '''\n",
        "\n",
        "    # image statistics, calculated in advance as averages across the full\n",
        "    # training data set\n",
        "    IMAGE_MEANS = (\n",
        "        (121.03431026287558, 82.52572736507886, 81.92368178210943),     # IR-R-G tiles\n",
        "        (285.34753853934154),                                           # DSM\n",
        "        (31.005143030549313)                                            # nDSM\n",
        "    )\n",
        "    IMAGE_STDS = (\n",
        "        (54.21029197978022, 38.434924159900554, 37.040640374137475),    # IR-R-G tiles\n",
        "        (6.485453035150256),                                            # DSM\n",
        "        (36.040236155124326)                                            # nDSM\n",
        "    )\n",
        "\n",
        "\n",
        "    # label class names\n",
        "    LABEL_CLASSES = (\n",
        "        'Impervious', 'Buildings', 'Low Vegetation', 'Tree', 'Car', 'Clutter'\n",
        "    )\n",
        "\n",
        "\n",
        "    def __init__(self, data_root, transform = None):\n",
        "        '''\n",
        "            Dataset class constructor. Here we initialize the dataset instance\n",
        "            and retrieve file names (and other metadata, if present) for all the\n",
        "            images and labels (ground truth semantic segmentation maps).\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        self.data_root = data_root\n",
        "\n",
        "        # find all images. In our case they are listed in a CSV file called\n",
        "        # \"fileList.csv\" under the \"data_root\"\n",
        "        with open(os.path.join(self.data_root, 'fileList.csv'), 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        self.transform = transform\n",
        "        \n",
        "        # parse CSV lines into data tokens: first column is the label file, the\n",
        "        # remaining ones are the image files\n",
        "        self.data = []\n",
        "        for line in lines[1:]:      # skip header\n",
        "            self.data.append(line.strip().split(','))\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "            This function tells the Data Loader how many images there are in\n",
        "            this dataset.\n",
        "        '''\n",
        "        return len(self.data)\n",
        "\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "            Here's where we load, prepare, and convert the images and\n",
        "            segmentation mask for the data element at the given \"idx\".\n",
        "        '''\n",
        "        item = self.data[idx]\n",
        "        print(item)\n",
        "        # load segmentation mask\n",
        "        labels_true = Image.open(os.path.join(self.data_root, 'labels', item[0]))\n",
        "        labels = np.array(labels_true, dtype=np.int64)\n",
        "        print(labels)\n",
        "\n",
        "        # load all images \n",
        "        images = [Image.open(os.path.join(self.data_root, 'images', i)) for i in item[1:]]    \n",
        "\n",
        "        images2 = images.copy()\n",
        "        for i in range(len(images)):  \n",
        "            img = images[i]\n",
        "            img = np.array(img, dtype=np.float32)                 \n",
        "            img2 = (img - self.IMAGE_MEANS[i]) / self.IMAGE_STDS[i]      # normalize\n",
        "            images[i] = img2\n",
        "            \n",
        "        \n",
        "        img_IRRG = np.array(images[0], dtype=np.float32)     \n",
        "        img_DSM = np.array(images[1], dtype=np.float32)  \n",
        "        img_nDSM = np.array(images[2],dtype = np.float32)   \n",
        "        images_all = np.dstack((img_IRRG,img_DSM,img_nDSM))\n",
        "        label = labels\n",
        "        \n",
        "        \n",
        "        aug_images = []\n",
        "        aug_labels = []\n",
        "        if self.transform is not None:\n",
        "            if augmentation_visu:  \n",
        "              N = 25\n",
        "            else:\n",
        "              N = 1\n",
        "            for k in range(N):\n",
        "              aug_img = images_all\n",
        "              aug_label = np.array(labels_true)\n",
        "              augmented = self.transform(image=aug_img, mask = labels)\n",
        "              augmented_image = augmented['image']#.permute(2,0,1)\n",
        "              augmented_label = augmented['mask']\n",
        "              aug_labels.append(augmented_label)\n",
        "              aug_images.append(augmented_image)\n",
        "        else:\n",
        "          aug_images.append(images_all)\n",
        "          aug_labels.append(label)\n",
        "\n",
        "\n",
        "        aug_images_tensor = []\n",
        "        for i in range(len(aug_images)):\n",
        "          if isinstance(aug_images[i],torch.Tensor):\n",
        "            aug_images_tensor.append(aug_images[i].float())\n",
        "          else:\n",
        "            aug_images_tensor.append(torch.from_numpy(aug_images[i]).float())\n",
        "        for i in range(len(aug_images_tensor)): \n",
        "          order = aug_images_tensor[i].size(0)\n",
        "          if order != 5:\n",
        "            aug_images_tensor[i] = aug_images_tensor[i].permute(2,0,1)\n",
        "\n",
        "        aug_label_tensor = []\n",
        "        for i in range(len(aug_labels)):\n",
        "          if isinstance(aug_labels[i],torch.Tensor):\n",
        "            aug_label_tensor.append(aug_labels[i])\n",
        "          else:\n",
        "            aug_label_tensor.append(torch.from_numpy(aug_labels[i]).long())\n",
        "\n",
        "\n",
        "        tensors = [T.ToTensor()(i) for i in images]\n",
        "        tensors = torch.cat(tensors, dim=0).float()         \n",
        "        \n",
        "        labels = torch.from_numpy(labels).long()        \n",
        "\n",
        "        # Decide wehter or not multiple augmented iamges are shown for the user\n",
        "        if augmentation_visu:\n",
        "            return aug_images_tensor,aug_label_tensor\n",
        "        else:\n",
        "            return aug_images_tensor[0],aug_label_tensor[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# we also create a function for the data loader here (see Section 2.6 in Exercise 6)\n",
        "def load_dataloader(batch_size, split, transform):\n",
        "  return DataLoader(\n",
        "      VaihingenDataset(os.path.join(data_root, split), transform),\n",
        "      batch_size=batch_size,\n",
        "      shuffle=(split=='train'),       # we shuffle the image order for the training dataset\n",
        "      num_workers=2                   # perform data loading with two CPU threads\n",
        "  )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "027spL6vWdQH"
      },
      "source": [
        "Image visualisation from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re6hs-8gqn7U"
      },
      "outputs": [],
      "source": [
        "  # visualise\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import albumentations as A\n",
        "import cv2\n",
        "\n",
        "#discrete color scheme\n",
        "cMap = ListedColormap(['black', 'grey', 'lawngreen', 'darkgreen', 'orange', 'red'])     \n",
        "#  'Impervious', 'Buildings', 'Low Vegetation', 'Tree', 'Car', 'Clutter'\n",
        "\n",
        "dataset_train = VaihingenDataset(os.path.join(data_root, 'train'), transform=train_transform)\n",
        "dataset_visu =VaihingenDataset(os.path.join(data_root, 'val'), transform=train_transform)\n",
        "\n",
        "# Decide wether or not the augmentations are shown\n",
        "augmentation_visu = False\n",
        "\n",
        "# 1 is a good image which contains \"complex\" scene\n",
        "aug_data, aug_trg = dataset_visu.__getitem__(1)\n",
        "print(np.unique(aug_trg.squeeze().numpy()))\n",
        "visualise = True\n",
        "if augmentation_visu:\n",
        "    for i in range(len(aug_data)):\n",
        "        data_augmented = aug_data[i] \n",
        "        label_augmented = aug_trg[i]\n",
        "        f, axarr = plt.subplots(nrows=1,ncols=4)\n",
        "        plt.sca(axarr[0]); \n",
        "        plt.imshow(data_augmented[:3,...].permute(1,2,0).numpy()); plt.title('NIR-R-G')\n",
        "        plt.sca(axarr[1]); \n",
        "        plt.imshow(data_augmented[3,...].squeeze().numpy()); plt.title('DSM')\n",
        "        plt.sca(axarr[2]); \n",
        "        plt.imshow(data_augmented[4,...].squeeze().numpy()); plt.title('nDSM')\n",
        "        plt.sca(axarr[3]); \n",
        "        cax = plt.imshow(label_augmented.squeeze().numpy(), cmap=cMap)                # target: segmentation mask\n",
        "        cbar = f.colorbar(cax, ticks=list(range(len(dataset_train.LABEL_CLASSES))))\n",
        "        cbar.ax.set_yticklabels(list(dataset_train.LABEL_CLASSES))\n",
        "        plt.title('Target: segmentation mask')\n",
        "        plt.show()\n",
        "else:\n",
        "    data_augmented = aug_data\n",
        "    label_augmented = aug_trg\n",
        "    f, axarr = plt.subplots(nrows=1,ncols=4)\n",
        "    plt.sca(axarr[0]); \n",
        "    plt.imshow(data_augmented[:3,...].permute(1,2,0).numpy()); plt.title('NIR-R-G')\n",
        "    plt.sca(axarr[1]); \n",
        "    plt.imshow(data_augmented[3,...].squeeze().numpy()); plt.title('DSM')\n",
        "    plt.sca(axarr[2]); \n",
        "    plt.imshow(data_augmented[4,...].squeeze().numpy()); plt.title('nDSM')\n",
        "    plt.sca(axarr[3]); \n",
        "    cax = plt.imshow(label_augmented.squeeze().numpy(), cmap=cMap)                # target: segmentation mask\n",
        "    cbar = f.colorbar(cax, ticks=list(range(len(dataset_train.LABEL_CLASSES))))\n",
        "    cbar.ax.set_yticklabels(list(dataset_train.LABEL_CLASSES))\n",
        "    plt.title('Target: segmentation mask')\n",
        "    plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IuXOGpMfldwb"
      },
      "source": [
        "The following cell is here to compute the class imbalance. It must be done only when first initialising the dataset. For this dataset, the results have been reported further and does not need to be run.\n",
        "In order to run it *compute_class_imbalance* must be set to True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2eylIDsKhZD3"
      },
      "outputs": [],
      "source": [
        "compute_class_imbalance = False\n",
        "if compute_class_imbalance:  \n",
        "  import os\n",
        "  %matplotlib inline\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "  from matplotlib.colors import ListedColormap\n",
        "  #import albumentations as A\n",
        "  import cv2\n",
        "  from tqdm import tqdm\n",
        "\n",
        "  #discrete color scheme\n",
        "  cMap = ListedColormap(['black', 'grey', 'lawngreen', 'darkgreen', 'orange', 'red'])     #  'Impervious', 'Buildings', 'Low Vegetation', 'Tree', 'Car', 'Clutter'\n",
        "\n",
        "  dataset_train = VaihingenDataset(os.path.join(data_root, 'train'))\n",
        "\n",
        "  nbr_class = np.zeros(6)\n",
        "\n",
        "  for k in range(len(dataset_train)):\n",
        "    _, aug_trg = dataset_train.__getitem__(k)\n",
        "    aug_trg.numpy()\n",
        "    for i in range(np.shape(aug_trg)[0]):\n",
        "      for j in range(np.shape(aug_trg)[1]):\n",
        "        label_px = aug_trg[i,j]\n",
        "        if label_px == 0:\n",
        "          nbr_class[0] += 1\n",
        "        elif label_px == 1:\n",
        "          nbr_class[1] += 1\n",
        "        elif label_px == 2:\n",
        "          nbr_class[2] += 1\n",
        "        elif label_px == 3:\n",
        "          nbr_class[3] += 1\n",
        "        elif label_px == 4:\n",
        "          nbr_class[4] += 1\n",
        "        elif label_px == 5:\n",
        "          nbr_class[5] += 1\n",
        "\n",
        "  # This shows the improtance of each class on the training dataset\n",
        "  class_imbalance = np.zeros(6)\n",
        "  print(nbr_class)\n",
        "  class_imbalance = nbr_class/np.sum(nbr_class)\n",
        "  print(class_imbalance)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MdvjBmx3mVvm"
      },
      "source": [
        "Class imbalance for the chosen dataset and train set. This allows to avoid running the previous cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xhH5-7LF9_MD"
      },
      "outputs": [],
      "source": [
        "if not compute_class_imbalance:\n",
        "  class_imbalance = [0.37021928, 0.22978685, 0.18439881, 0.19673866, 0.01098469, 0.0078717]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yS8VRYM-wsHh"
      },
      "source": [
        "## 3. Model\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n6A1fa7YnW5r"
      },
      "source": [
        "More details about the model blocks and implementation can be found on the report."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9CW2IpiswtKW"
      },
      "source": [
        "This part focus on the model selection. The weighted terms meand that the class imbalance are being taken into account when training.\n",
        "\n",
        "Please select only one of the models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "55x1ee5kp9iO"
      },
      "outputs": [],
      "source": [
        "choose_Hypercolumn = True\n",
        "choose_Hypercolumn_weighted = False\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kDk7h1HXm3n3"
      },
      "source": [
        "## 3.1. Hypercolumns model\n",
        "Initialise the Hypercolumns model's blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hdoIpSFpWdQJ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Hypercolumn(nn.Module):\n",
        "\n",
        "    def __init__(self,in_ch,out_ch):\n",
        "        super(Hypercolumn, self).__init__()\n",
        "\n",
        "        #TODO: define your architecture and forward pass here\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 32, kernel_size=5, stride=4),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
        "            nn.BatchNorm2d(num_features=32),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=4),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=5, stride=2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.block4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Conv2d(485, 256, kernel_size=1, stride=1),           # 485 = 256 + 128 + 64 + 32 + 5 (input bands)\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, out_ch, kernel_size=1, stride=1)\n",
        "        )\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        #TODO\n",
        "        upsample = nn.Upsample(size=(x.size(2), x.size(3)))\n",
        "        x1 = self.block1(x)\n",
        "        x2 = self.block2(x1)\n",
        "        x3 = self.block3(x2)\n",
        "        x4 = self.block4(x3)\n",
        "\n",
        "        hypercol = torch.cat(\n",
        "            (x, upsample(x1), upsample(x2), upsample(x3), upsample(x4)),\n",
        "            dim=1\n",
        "        )\n",
        "        return self.final(hypercol)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g6frSNDmnAES"
      },
      "source": [
        "Checks that the model is alright"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "p2HyHn41mXRe"
      },
      "outputs": [],
      "source": [
        "dataloader_train = load_dataloader(2, 'train',train_transform)\n",
        "n_channels = 5  # NIR - R - G - DSM - nDSM\n",
        "n_classes = 6  #'Impervious', 'Buildings', 'Low Vegetation', 'Tree', 'Car', 'Clutter'\n",
        "\n",
        "model = Hypercolumn(n_channels,n_classes)\n",
        "\n",
        "data, _ = iter(dataloader_train).__next__()\n",
        "pred = model(data)\n",
        "\n",
        "assert pred.size(1) == len(dataset_train.LABEL_CLASSES), f'ERROR: invalid number of model output channels (should be # classes {len(dataset_train.LABEL_CLASSES)}, got {pred.size(1)})'\n",
        "assert pred.size(2) == data.size(2), f'ERROR: invalid spatial height of model output (should be {data.size(2)}, got {pred.size(2)})'\n",
        "assert pred.size(3) == data.size(3), f'ERROR: invalid spatial width of model output (should be {data.size(3)}, got {pred.size(3)})'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vdt2JF9rnLmz"
      },
      "source": [
        "\n",
        "Checks that the model is alright"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Ys65zrOYiQ"
      },
      "source": [
        "## 3.4. Model check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV8-dxhaNRMJ",
        "outputId": "8084a163-e099-464d-89f7-ac8acfe40c66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: Hypercolumn\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 99, 99]           4,032\n",
            "         MaxPool2d-2           [-1, 32, 98, 98]               0\n",
            "       BatchNorm2d-3           [-1, 32, 98, 98]              64\n",
            "              ReLU-4           [-1, 32, 98, 98]               0\n",
            "            Conv2d-5           [-1, 64, 24, 24]          51,264\n",
            "         MaxPool2d-6           [-1, 64, 23, 23]               0\n",
            "       BatchNorm2d-7           [-1, 64, 23, 23]             128\n",
            "              ReLU-8           [-1, 64, 23, 23]               0\n",
            "            Conv2d-9          [-1, 128, 10, 10]         204,928\n",
            "        MaxPool2d-10            [-1, 128, 9, 9]               0\n",
            "      BatchNorm2d-11            [-1, 128, 9, 9]             256\n",
            "             ReLU-12            [-1, 128, 9, 9]               0\n",
            "           Conv2d-13            [-1, 256, 7, 7]         295,168\n",
            "        MaxPool2d-14            [-1, 256, 6, 6]               0\n",
            "      BatchNorm2d-15            [-1, 256, 6, 6]             512\n",
            "             ReLU-16            [-1, 256, 6, 6]               0\n",
            "           Conv2d-17        [-1, 256, 400, 400]         124,416\n",
            "      BatchNorm2d-18        [-1, 256, 400, 400]             512\n",
            "             ReLU-19        [-1, 256, 400, 400]               0\n",
            "           Conv2d-20          [-1, 6, 400, 400]           1,542\n",
            "================================================================\n",
            "Total params: 682,822\n",
            "Trainable params: 682,822\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 3.05\n",
            "Forward/backward pass size (MB): 955.95\n",
            "Params size (MB): 2.60\n",
            "Estimated Total Size (MB): 961.61\n",
            "----------------------------------------------------------------\n",
            "Model: Hypercolumn\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model_sum  = model.to(device='cuda')\n",
        "\n",
        "print(\"Model:\",model.__class__.__name__)\n",
        "summary(model_sum,(5, crop_height, crop_width),device='cuda')\n",
        "print(\"Model:\",model.__class__.__name__)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vqS6imI8WdQK"
      },
      "source": [
        "## 4. Model training function initialisation\n",
        "\n",
        "This section trains the chosen model with/without weighted loss function as defined before."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WDGlVJTdOppd"
      },
      "source": [
        "## 4.1. Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfPptFtEWdQL",
        "outputId": "57af9e44-dfb9-421b-bcab-cf031dc5a28a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no weight\n"
          ]
        }
      ],
      "source": [
        "# Loss function chose\n",
        "if choose_Hypercolumn_weighted:\n",
        "  class_imbalance = [0.37021928, 0.22978685, 0.18439881, 0.19673866, 0.01098469, 0.0078717]\n",
        "  normedWeights = [(1 - x) for x in class_imbalance]\n",
        "  normedWeights = torch.FloatTensor(normedWeights).to('cuda')\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss(weight=normedWeights)\n",
        "else:\n",
        "  criterion = nn.CrossEntropyLoss() \n",
        "  print(\"no weight\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IgZL1H5xO9XD"
      },
      "source": [
        "## 4.3. General loss function setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tYKOHgsIWdQL"
      },
      "outputs": [],
      "source": [
        "# Sets up the optimiser\n",
        "\n",
        "from torch.optim import SGD\n",
        "\n",
        "def setup_optimiser(model, learning_rate, momentum, weight_decay):\n",
        "  return SGD(\n",
        "    model.parameters(),\n",
        "    learning_rate,\n",
        "    momentum,\n",
        "    weight_decay\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "T-qunNJrWdQM"
      },
      "outputs": [],
      "source": [
        "# Training block implementation\n",
        "from tqdm.notebook import trange \n",
        "\n",
        "\n",
        "def train_epoch(data_loader, model, optimiser, device):\n",
        "\n",
        "  # set model to training mode. This is important because some layers behave differently during training and testing\n",
        "  model.train(True)\n",
        "  model.to(device)\n",
        "\n",
        "  # stats\n",
        "  loss_total = 0.0\n",
        "  oa_total = 0.0\n",
        "\n",
        "  # iterate over dataset\n",
        "  pBar = trange(len(data_loader))\n",
        "  for idx, (data, target) in enumerate(data_loader):\n",
        "\n",
        "    #TODO: implement the training step here. Check the introductory slides if you need help.\n",
        "\n",
        "    # put data and target onto correct device\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # reset gradients\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    # forward pass\n",
        "    pred = model(data)\n",
        "\n",
        "    # loss\n",
        "    loss = criterion(pred, target)\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # parameter update\n",
        "    optimiser.step()\n",
        "\n",
        "    # stats update\n",
        "    loss_total += loss.item()\n",
        "    oa_total += torch.mean((pred.argmax(1) == target).float()).item()\n",
        "\n",
        "    # format progress bar\n",
        "    pBar.set_description('Loss: {:.2f}, OA: {:.2f}'.format(\n",
        "      loss_total/(idx+1),\n",
        "      100 * oa_total/(idx+1)\n",
        "    ))\n",
        "    pBar.update(1)\n",
        "  \n",
        "  pBar.close()\n",
        "\n",
        "  # normalise stats\n",
        "  loss_total /= len(data_loader)\n",
        "  oa_total /= len(data_loader)\n",
        "\n",
        "  return model, loss_total, oa_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WygIf_7XWdQM"
      },
      "outputs": [],
      "source": [
        "def validate_epoch(data_loader, model, device):      \n",
        "\n",
        "  # set model to evaluation mode\n",
        "  model.train(False)\n",
        "  model.to(device)\n",
        "\n",
        "  # stats\n",
        "  loss_total = 0.0\n",
        "  oa_total = 0.0\n",
        "\n",
        "  # iterate over dataset\n",
        "  pBar = trange(len(data_loader))\n",
        "  for idx, (data, target) in enumerate(data_loader):\n",
        "    with torch.no_grad():\n",
        "\n",
        "\n",
        "      # put data and target onto correct device\n",
        "      data, target = data.to(device), target.to(device)\n",
        "\n",
        "      # forward pass\n",
        "      pred = model(data)\n",
        "\n",
        "      # loss\n",
        "      loss = criterion(pred, target)\n",
        "\n",
        "      # stats update\n",
        "      loss_total += loss.item()\n",
        "      oa_total += torch.mean((pred.argmax(1) == target).float()).item()\n",
        "\n",
        "      # format progress bar\n",
        "      pBar.set_description('Loss: {:.2f}, OA: {:.2f}'.format(\n",
        "        loss_total/(idx+1),\n",
        "        100 * oa_total/(idx+1)\n",
        "      ))\n",
        "      pBar.update(1)\n",
        "\n",
        "  pBar.close()\n",
        "\n",
        "  # normalise stats\n",
        "  loss_total /= len(data_loader)\n",
        "  oa_total /= len(data_loader)\n",
        "\n",
        "  return loss_total, oa_total"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5KB1HhuzWdQN"
      },
      "source": [
        "The following section loads the correct models weights according to the choice of model and weighted loss or not"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5bqGdb82FBFD"
      },
      "source": [
        "### 2. Hypercolumns with or without weighted Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_R-rZeiLBt8w"
      },
      "outputs": [],
      "source": [
        "if choose_Hypercolumn: \n",
        "  \n",
        "  os.makedirs('gdrive/MyDrive/SegVai/Hypercolumn', exist_ok=True)\n",
        "\n",
        "  def load_model(n_channels=5, n_classes=6,epoch='latest'):\n",
        "    model = Hypercolumn(n_channels, n_classes)\n",
        "    modelStates = glob.glob('gdrive/MyDrive/SegVais/Hypercolumn/*.pth')\n",
        "    if len(modelStates) and (epoch == 'latest' or epoch > 0):\n",
        "      modelStates = [int(m.replace('gdrive/MyDrive/SegVai/Hypercolumn/','').replace('.pth', '')) for m in modelStates]\n",
        "      if epoch == 'latest':\n",
        "        epoch = max(modelStates)\n",
        "      stateDict = torch.load(open(f'gdrive/MyDrive/SegVai/Hypercolumn/{epoch}.pth', 'rb'), map_location='cpu')\n",
        "      model.load_state_dict(stateDict)\n",
        "    else:\n",
        "      # fresh model\n",
        "      epoch = 0\n",
        "    return model, epoch\n",
        "\n",
        "\n",
        "  def save_model(model, epoch):\n",
        "    torch.save(model.state_dict(), open(f'gdrive/MyDrive/SegVai/Hypercolumn/{epoch}.pth', 'wb'))\n",
        "\n",
        "elif choose_Hypercolumn_weighted: \n",
        "  \n",
        "  os.makedirs('gdrive/MyDrive/SegVai/Hypercolumn_balance', exist_ok=True)\n",
        "\n",
        "  def load_model(n_channels=5, n_classes=6,epoch='latest'):\n",
        "    model = Hypercolumn(n_channels, n_classes)\n",
        "    modelStates = glob.glob('gdrive/MyDrive/SegVai/Hypercolumn_balance/*.pth')\n",
        "    if len(modelStates) and (epoch == 'latest' or epoch > 0):\n",
        "      modelStates = [int(m.replace('gdrive/MyDrive/SegVai/Hypercolumn_balance/','').replace('.pth', '')) for m in modelStates]\n",
        "      if epoch == 'latest':\n",
        "        epoch = max(modelStates)\n",
        "      stateDict = torch.load(open(f'gdrive/MyDrive/SegVai/Hypercolumn_balance/{epoch}.pth', 'rb'), map_location='cpu')\n",
        "      model.load_state_dict(stateDict)\n",
        "    else:\n",
        "      # fresh model\n",
        "      epoch = 0\n",
        "    return model, epoch\n",
        "\n",
        "\n",
        "  def save_model(model, epoch):\n",
        "    torch.save(model.state_dict(), open(f'gdrive/MyDrive/SegVai/Hypercolumn_balance/{epoch}.pth', 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "na44bTOEFGaf"
      },
      "source": [
        "## 5. Training model cell"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XURAJfZPvL8K"
      },
      "source": [
        "The next cell determines if we want to train or just visualise resuls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vjmEoXEuvJs8"
      },
      "outputs": [],
      "source": [
        "training = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6nYSpomWdQO"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "if training:\n",
        "  # define hyperparameters\n",
        "  device = 'cuda'\n",
        "  start_epoch =  0    # set to 0 to start from scratch again or to 'latest' to continue training from saved checkpoint\n",
        "  batch_size = 2\n",
        "  learning_rate = 0.1\n",
        "  momentum = 0.5\n",
        "  weight_decay = 0.001\n",
        "  num_epochs = 30\n",
        "  n_channels = 5  # NIR - R - G - DSM - nDSM\n",
        "  n_classes = 6  #'Impervious', 'Buildings', 'Low Vegetation', 'Tree', 'Car', 'Clutter'\n",
        "\n",
        "\n",
        "\n",
        "  # initialise data loaders\n",
        "  dl_train = load_dataloader(batch_size, 'train',train_transform)\n",
        "\n",
        "  dl_val = load_dataloader(batch_size, 'val',test_transform)\n",
        "\n",
        "  # load model\n",
        "\n",
        "  model, epoch = load_model(n_channels, n_classes,epoch=start_epoch)\n",
        "  optimi = setup_optimiser(model, learning_rate, momentum, weight_decay)\n",
        "\n",
        "  # do epochs\n",
        "  while epoch < num_epochs:\n",
        "\n",
        "    # training\n",
        "    model, loss_train, oa_train = train_epoch(dl_train, model, optimi, device)\n",
        "\n",
        "    # validation\n",
        "    loss_val, oa_val = validate_epoch(dl_val, model, device)\n",
        "\n",
        "    # print stats\n",
        "    print('[Ep. {}/{}] Loss train: {:.2f}, val: {:.2f}; OA train: {:.2f}, val: {:.2f}'.format(\n",
        "        epoch+1, num_epochs,\n",
        "        loss_train, loss_val,\n",
        "        100*oa_train, 100*oa_val\n",
        "    ))\n",
        "\n",
        "    # save model\n",
        "    epoch += 1\n",
        "    if epoch % 10 == 0:\n",
        "      save_model(model, epoch)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ABNjPz90opu5"
      },
      "source": [
        "## 6. Model prediction & Metrics evaluation\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kyPxNX8jpPt5"
      },
      "source": [
        "### 1. Basic function implementation for visualisation and Metrics computations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pR4Vzv2Oo1Iw"
      },
      "source": [
        "The following function implements the diverse metrics used. Respectively: \n",
        "F-Score, Cohen's kappa, Overall Accuracy, User's Accuracy, Producer's Accuracy, Intersection over Union, recall and precision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "U6yZ4urQ6gqm"
      },
      "outputs": [],
      "source": [
        "from math import isnan\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def compute_metrics(cm):\n",
        "    '''\n",
        "    Adapted from:\n",
        "        https://github.com/davidtvs/PyTorch-ENet/blob/master/metric/iou.py\n",
        "        https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/metrics.py#L2716-L2844\n",
        "    '''\n",
        "    \n",
        "    sum_over_row = cm.sum(axis=0)\n",
        "    sum_over_col = cm.sum(axis=1)\n",
        "    true_positives = np.diag(cm)\n",
        "\n",
        "    # sum_over_row + sum_over_col = 2 * true_positives + false_positives + false_negatives.\n",
        "    denominator = sum_over_row + sum_over_col - true_positives\n",
        "    \n",
        "    iou = true_positives / denominator\n",
        "\n",
        "    precision = true_positives/sum_over_col\n",
        "\n",
        "    recall = true_positives/sum_over_row\n",
        "    \n",
        "    f1 = 2* (precision*recall)/(precision + recall)\n",
        "    for l in range(len(f1)):\n",
        "      if math.isnan(f1[l]):\n",
        "        f1[l] = 0.00\n",
        "      if math.isnan(iou[l]):\n",
        "        iou[l] = 0.00\n",
        "    N = sum(sum_over_row)\n",
        "    tmp = 0\n",
        "    for i in range(len(sum_over_row)):\n",
        "      tmp += sum_over_row[i]*sum_over_col[i]\n",
        "    nominator = N *(sum(true_positives.flatten())) - tmp\n",
        "    denominator = N*N - tmp\n",
        "    kappa = (nominator)/(denominator)\n",
        "\n",
        "    PA = true_positives/sum_over_row\n",
        "    UA = true_positives/sum_over_col\n",
        "    for l in range(len(PA)):\n",
        "      if math.isnan(PA[l]):\n",
        "        PA[l] = 0.00\n",
        "      if math.isnan(UA[l]):\n",
        "        UA[l] = 0.00\n",
        "    OA = sum(true_positives)/N\n",
        "    return iou, recall, precision, f1, kappa, OA,PA, UA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YvS9oRpUHm8W"
      },
      "outputs": [],
      "source": [
        "def check_cmap(pred, old_cmap):\n",
        "  new_cmap = old_cmap.copy()\n",
        "  uniqueness = np.unique(pred.cpu().numpy())\n",
        "  for k in range(len(new_cmap)):\n",
        "    if k not in uniqueness:\n",
        "        new_cmap.remove(old_cmap[k])\n",
        "  return new_cmap"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw4ic6LrEFRk"
      },
      "source": [
        "### 2. Model output visualisation and metrics evaluation\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xPiuiFyQECEd"
      },
      "source": [
        "#### 1. Hypercolumns without weighted Loss "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqHyYUcyA2iG"
      },
      "outputs": [],
      "source": [
        "if choose_Hypercolumn:\n",
        "  device = 'cuda'\n",
        "  import warnings\n",
        "  %matplotlib inline\n",
        "  import matplotlib.pyplot as plt\n",
        "  import math\n",
        "  import pandas as pd\n",
        "  from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
        "  base_cmp = ['black', 'grey', 'lawngreen', 'darkgreen', 'orange', 'red']\n",
        "  cMap = ListedColormap(base_cmp)     #  'Impervious', 'Buildings', 'Low Vegetation', 'Tree', 'Car', 'Clutter'\n",
        "  x = ['black', 'grey', 'lawngreen', 'darkgreen', 'orange', 'red']\n",
        "  cMap2 = ListedColormap(x)\n",
        "\n",
        "  warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "  def visualize(dataLoader,n_channels, n_classes, epochs, show_metrics = False, numImages=5):\n",
        "    models = [load_model(n_channels, n_classes,e)[0] for e in epochs]\n",
        "    numModels = len(models)\n",
        "    for idx, (data, labels) in enumerate(dataLoader):\n",
        "      list_gt_labels = []\n",
        "      if idx == numImages:\n",
        "        break\n",
        "      if idx == 0 or idx == 1:\n",
        "        continue\n",
        "\n",
        "      f, ax = plt.subplots(nrows=1, ncols=numModels+1, figsize = (10, 10))\n",
        "\n",
        "      list_gt_labels.append(labels[0,...].cpu().numpy().flatten())\n",
        "\n",
        "      # plot ground truth\n",
        "      plt.sca(ax[0]);\n",
        "      cax = plt.imshow(labels.squeeze().cpu().numpy(), cmap=cMap2)\n",
        "      cbar = f.colorbar(cax, ticks=list(range(6)),orientation='horizontal')\n",
        "      cbar.ax.set_xticklabels(['Impervious', 'Buildings', 'Low Vegetation', 'Tree', 'Car', 'Clutter'],rotation =45)\n",
        "      plt.title('Ground Truth')   \n",
        "      \n",
        "      \n",
        "      \n",
        "      ax[0].axis('off')\n",
        "      if idx == 0:\n",
        "        ax[0].set_title('Ground Truth')\n",
        "      conf_matrix = []\n",
        "      accuracy = []\n",
        "      for mIdx, model in enumerate(models):\n",
        "        list_predictions = []\n",
        "        model = model.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "          pred = model(data.to(device))\n",
        "          # get the label (i.e., the maximum position for each pixel along the class dimension)\n",
        "          yhat = torch.argmax(pred, dim=1)\n",
        "\n",
        "          list_predictions.append(yhat[0,...].cpu().numpy().flatten())     \n",
        "          all_predictions = np.concatenate(list_predictions)\n",
        "          all_gt_labels = np.concatenate(list_gt_labels)\n",
        "          accuracy.append(accuracy_score(all_gt_labels, all_predictions))\n",
        "          conf_matrix.append(confusion_matrix(all_gt_labels, all_predictions,labels=[0,1, 2, 3,4,5]))    \n",
        "          new_cmap = check_cmap(yhat, base_cmp)\n",
        "          plt.sca(ax[mIdx+1]); \n",
        "          cax = plt.imshow(yhat[0,...].cpu().numpy(), cmap=ListedColormap(new_cmap))     # target: segmentation mask\n",
        "\n",
        "\n",
        "\n",
        "      if show_metrics:\n",
        "        if numModels == 1:\n",
        "          _, ax = plt.subplots(nrows=1, ncols=numModels, figsize = (7, 7))\n",
        "        else:\n",
        "          _, ax = plt.subplots(nrows=1, ncols=numModels, figsize = (15, 15))\n",
        "        for mIdx, model in enumerate(models):\n",
        "          conf_matrix_one = conf_matrix[mIdx]\n",
        "          if numModels == 1:\n",
        "            ax.matshow(conf_matrix_one, cmap=plt.cm.Blues, alpha=0.5)\n",
        "          else:\n",
        "            ax[mIdx].matshow(conf_matrix_one, cmap=plt.cm.Blues, alpha=0.5)\n",
        "\n",
        "          \n",
        "\n",
        "          iou, recall, precision, f1, kappa, OA,PA, UA = compute_metrics(conf_matrix_one)\n",
        "          \n",
        "          \n",
        "          for i in range(conf_matrix_one.shape[0]):\n",
        "            for j in range(conf_matrix_one.shape[1]):\n",
        "              if math.isnan(conf_matrix_one[i,j]):\n",
        "                conf_matrix_one[i,j] = 0\n",
        "              if numModels == 1:\n",
        "                ax.text(x=j, y=i,s=conf_matrix_one[i, j], va='center', ha='center', size='x-large')\n",
        "              else:\n",
        "                ax[mIdx].text(x=j, y=i,s=conf_matrix_one[i, j], va='center', ha='center', size='x-large')\n",
        "            \n",
        "            if numModels == 1:\n",
        "              ax.set_xlabel('Predictions', fontsize=18)\n",
        "              ax.set_ylabel('Ground Truth', fontsize=18)\n",
        "              ax.set_title('Confusion Matrix', fontsize=18)\n",
        "            else:\n",
        "              ax[mIdx].set_xlabel('Predictions', fontsize=18)\n",
        "              ax[mIdx].set_ylabel('Ground Truth', fontsize=18)\n",
        "              ax[mIdx].set_title('Confusion Matrix', fontsize=18)\n",
        "          if idx == 2 and (epochs[mIdx] == 'latest' or epochs[mIdx] == 30):\n",
        "                print(\"Epoch:\",epochs[mIdx])\n",
        "                print(\"F1\",f1)\n",
        "                print(\"IoU\",iou)\n",
        "                print(\"Kappa\",kappa)\n",
        "                print(\"OA\",OA)\n",
        "                print(\"UA\",UA)\n",
        "                print(\"PA\",PA)\n",
        "                print(\"Mean FScore\",sum(f1)/len(f1))\n",
        "                print(\"Mean IoU\",sum(iou)/len(iou))\n",
        "                print(\"Mean UA\",sum(UA)/len(UA))\n",
        "                print(\"Mean PA\",sum(PA)/len(PA))\n",
        "                print(\"===========================\")\n",
        "        \n",
        "  # visualize predictions for a number of epochs\n",
        "  dl_val_single = load_dataloader(1, 'val',report_transform)\n",
        "\n",
        "  # load model states at different epochs\n",
        "  epochs = ['latest']                                          #TODO: modify this vector according to your wishes, resp. for how many model states you have trained\n",
        "  n_channels = 5  # NIR - R - G - DSM - nDSM\n",
        "  n_classes = 6  #'Impervious', 'Buildings', ' Low Vegetation', 'Tree', 'Car', 'Clutter'\n",
        "\n",
        "  visualize(dl_val_single, n_channels, n_classes,epochs, show_metrics = True, numImages=3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AcW1phk7pd3u"
      },
      "source": [
        "#### 2. Hypercolumns with weighted Loss "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pji5b_Z9Iqz-"
      },
      "outputs": [],
      "source": [
        "if choose_Hypercolumn_weighted:\n",
        "  device = 'cuda'\n",
        "  import warnings\n",
        "  %matplotlib inline\n",
        "  import matplotlib.pyplot as plt\n",
        "  import math\n",
        "  import pandas as pd\n",
        "  from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
        "  base_cmp = ['black', 'grey', 'lawngreen', 'darkgreen', 'orange', 'red']\n",
        "  cMap = ListedColormap(base_cmp)     #  'Impervious', 'Buildings', 'Low Vegetation', 'Tree', 'Car', 'Clutter'\n",
        "  x = ['black', 'grey', 'lawngreen', 'darkgreen', 'orange', 'red']\n",
        "  cMap2 = ListedColormap(x)\n",
        "\n",
        "  warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "  def visualize(dataLoader,n_channels, n_classes, epochs, show_metrics = False, numImages=5):\n",
        "    models = [load_model(n_channels, n_classes,e)[0] for e in epochs]\n",
        "    numModels = len(models)\n",
        "    for idx, (data, labels) in enumerate(dataLoader):\n",
        "      list_gt_labels = []\n",
        "      if idx == numImages:\n",
        "        break\n",
        "      if idx == 0 or idx == 1:\n",
        "        continue\n",
        "\n",
        "      f, ax = plt.subplots(nrows=1, ncols=numModels+1, figsize = (10, 10))\n",
        "\n",
        "      list_gt_labels.append(labels[0,...].cpu().numpy().flatten())\n",
        "\n",
        "      # plot ground truth\n",
        "      plt.sca(ax[0]);\n",
        "      cax = plt.imshow(labels.squeeze().cpu().numpy(), cmap=cMap2)\n",
        "      cbar = f.colorbar(cax, ticks=list(range(6)),orientation='horizontal')\n",
        "      cbar.ax.set_xticklabels(['Impervious', 'Buildings', 'Low Vegetation', 'Tree', 'Car', 'Clutter'],rotation ='45')\n",
        "      plt.title('Ground Truth')   \n",
        "      \n",
        "      \n",
        "      ax[0].axis('off')\n",
        "      if idx == 0:\n",
        "        ax[0].set_title('Ground Truth')\n",
        "      conf_matrix = []\n",
        "      accuracy = []\n",
        "      for mIdx, model in enumerate(models):\n",
        "        list_predictions = []\n",
        "        model = model.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "          pred = model(data.to(device))\n",
        "          # get the label (i.e., the maximum position for each pixel along the class dimension)\n",
        "          yhat = torch.argmax(pred, dim=1)\n",
        "\n",
        "          list_predictions.append(yhat[0,...].cpu().numpy().flatten())     \n",
        "          all_predictions = np.concatenate(list_predictions)\n",
        "          all_gt_labels = np.concatenate(list_gt_labels)\n",
        "          accuracy.append(accuracy_score(all_gt_labels, all_predictions))\n",
        "          conf_matrix.append(confusion_matrix(all_gt_labels, all_predictions,labels=[0,1, 2, 3,4,5]))    \n",
        "          new_cmap = check_cmap(yhat, base_cmp)\n",
        "          plt.sca(ax[mIdx+1]); \n",
        "          cax = plt.imshow(yhat[0,...].cpu().numpy(), cmap=ListedColormap(new_cmap))     # target: segmentation mask\n",
        "\n",
        "\n",
        "\n",
        "      if show_metrics:\n",
        "        if numModels == 1:\n",
        "          _, ax = plt.subplots(nrows=1, ncols=numModels, figsize = (7, 7))\n",
        "        else:\n",
        "          _, ax = plt.subplots(nrows=1, ncols=numModels, figsize = (15, 15))\n",
        "        for mIdx, model in enumerate(models):\n",
        "          conf_matrix_one = conf_matrix[mIdx]\n",
        "          if numModels == 1:\n",
        "            ax.matshow(conf_matrix_one, cmap=plt.cm.Blues, alpha=0.5)\n",
        "          else:\n",
        "            ax[mIdx].matshow(conf_matrix_one, cmap=plt.cm.Blues, alpha=0.5)\n",
        "\n",
        "          \n",
        "\n",
        "          iou, recall, precision, f1, kappa, OA,PA, UA = compute_metrics(conf_matrix_one)\n",
        "          \n",
        "          \n",
        "          for i in range(conf_matrix_one.shape[0]):\n",
        "            for j in range(conf_matrix_one.shape[1]):\n",
        "              if math.isnan(conf_matrix_one[i,j]):\n",
        "                conf_matrix_one[i,j] = 0\n",
        "              if numModels == 1:\n",
        "                ax.text(x=j, y=i,s=conf_matrix_one[i, j], va='center', ha='center', size='x-large')\n",
        "              else:\n",
        "                ax[mIdx].text(x=j, y=i,s=conf_matrix_one[i, j], va='center', ha='center', size='x-large')\n",
        "            \n",
        "            if numModels == 1:\n",
        "              ax.set_xlabel('Predictions', fontsize=18)\n",
        "              ax.set_ylabel('Ground Truth', fontsize=18)\n",
        "              ax.set_title('Confusion Matrix', fontsize=18)\n",
        "            else:\n",
        "              ax[mIdx].set_xlabel('Predictions', fontsize=18)\n",
        "              ax[mIdx].set_ylabel('Ground Truth', fontsize=18)\n",
        "              ax[mIdx].set_title('Confusion Matrix', fontsize=18)\n",
        "\n",
        "          if idx == 2 and (epochs[mIdx] == 'latest' or epochs[mIdx] == 30):\n",
        "                print(\"Epoch:\",epochs[mIdx])\n",
        "                print(\"F1\",f1)\n",
        "                print(\"IoU\",iou)\n",
        "                print(\"Kappa\",kappa)\n",
        "                print(\"OA\",OA)\n",
        "                print(\"UA\",UA)\n",
        "                print(\"PA\",PA)\n",
        "                print(\"Mean FScore\",sum(f1)/len(f1))\n",
        "                print(\"Mean IoU\",sum(iou)/len(iou))\n",
        "                print(\"Mean UA\",sum(UA)/len(UA))\n",
        "                print(\"Mean PA\",sum(PA)/len(PA))\n",
        "                print(\"===========================\")\n",
        "        \n",
        "  # visualize predictions for a number of epochs\n",
        "  dl_val_single = load_dataloader(1, 'val',report_transform)\n",
        "\n",
        "  # load model states at different epochs\n",
        "  epochs = ['latest']                                          #TODO: modify this vector according to your wishes, resp. for how many model states you have trained\n",
        "  n_channels = 5  # NIR - R - G - DSM - nDSM\n",
        "  n_classes = 6  #'Impervious', 'Buildings', ' Low Vegetation', 'Tree', 'Car', 'Clutter'\n",
        "\n",
        "  visualize(dl_val_single, n_channels, n_classes,epochs, show_metrics = True, numImages=3)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "d87036bccc21f2c1ac8988b25c74afd5927b201ca402dbbb169419cef9fe6115"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
